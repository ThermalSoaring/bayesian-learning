{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction\n",
    "\n",
    "Network used if n=3:\n",
    "<img src=\"PrototypeBN.png\" alt=\"BN\" width=\"50%\" />\n",
    "\n",
    "Below is all the code to create and parse the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Predict the next word of a sentence using a Bayesian Network. Learn the\n",
    "# probabilities from Gutenburg Project text documents.\n",
    "#\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import pathlib\n",
    "import operator\n",
    "import pomegranate\n",
    "import readline\n",
    "\n",
    "#\n",
    "# Clean up a paragraph\n",
    "#\n",
    "def cleanup(text):\n",
    "    text = text.strip()\n",
    "    # Convert dashes and line returns to spaces\n",
    "    text = re.sub('(--|\\n)', ' ', text)\n",
    "    # Only allow case-insensitive a-z in words\n",
    "    text = re.sub('[^a-zA-Z \\'-]+', '', text)\n",
    "    return text\n",
    "\n",
    "#\n",
    "# Find all the words and the next word from a Gutenburg Project text document\n",
    "# and use these to construct a Bayesian network\n",
    "#\n",
    "def constructNetworkFromFiles(filenames, number):\n",
    "    assert number > 0, \"n > 0\"\n",
    "\n",
    "    paragraphs = []\n",
    "\n",
    "    # Parse each of the files\n",
    "    for filename in filenames:\n",
    "        p = pathlib.Path(filename)\n",
    "        assert p.is_file(), \"Must specify valid file\"\n",
    "\n",
    "        # Get all the book data from this one file\n",
    "        data = \"\"\n",
    "\n",
    "        try:\n",
    "            with p.open() as f:\n",
    "                start = False\n",
    "                end   = False\n",
    "\n",
    "                for line in f:\n",
    "                    if re.match(\"^\\*\\*\\* END OF THIS PROJECT\", line):\n",
    "                        end = True\n",
    "\n",
    "                    # Only use this line if it's between the start and end\n",
    "                    if start and not end:\n",
    "                        data += line\n",
    "\n",
    "                    # Find the start and end of the document\n",
    "                    if re.match(\"^\\*\\*\\* START OF THIS PROJECT\", line):\n",
    "                        start = True\n",
    "        except OSError:\n",
    "            print(\"Could not open file\")\n",
    "\n",
    "        # Split by paragraphs\n",
    "        word_usage = [{} for i in range(0, number)]\n",
    "        paragraphs += data.split('\\n\\n')\n",
    "\n",
    "    # Using the paragraphs obtained from all the files, look for word sequences\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph:\n",
    "            # Split by words\n",
    "            words = cleanup(paragraph).split(' ')\n",
    "\n",
    "            # For each length sequence of words (i.e., if n=3, for a sequence\n",
    "            # of length 1, 2, and 3)\n",
    "            for num in range(1, number+1):\n",
    "                # Look at a sliding window of the specified number of words\n",
    "                # finding the frequency of each sequenc\n",
    "                for i in range(num, len(words)):\n",
    "                    s = ','.join([words[i-(num-j)] for j in range(0, num)])\n",
    "\n",
    "                    if s in word_usage[num-1]:\n",
    "                        word_usage[num-1][s] += 1\n",
    "                    else:\n",
    "                        word_usage[num-1][s] = 1\n",
    "\n",
    "    # Total number to divide by to compute the probabilities\n",
    "    total_freq = [sum(d.values()) for d in word_usage]\n",
    "\n",
    "    # Construct the network\n",
    "    word_distributions = []\n",
    "\n",
    "    # First word, only one that isn't a CPD. Just take the frequency and divide\n",
    "    # by the total to get probabilities.\n",
    "    word_distributions.append(pomegranate.DiscreteDistribution(\n",
    "        { key: value/total_freq[0] for (key, value) in word_usage[0].items()}))\n",
    "\n",
    "    # The other words (2, 3, etc.)\n",
    "    for i in range(1, number):\n",
    "        word_distributions.append(pomegranate.ConditionalProbabilityTable(\n",
    "            # CPT: word1, ..., this_word, prob(this_word)\n",
    "            [key.split(',')+[value] for (key, value) in word_usage[i].items()],\n",
    "            # depends on all words up to this word\n",
    "            [word_distributions[j] for j in range(0, i)]))\n",
    "\n",
    "    # Create all the nodes\n",
    "    nodes = []\n",
    "\n",
    "    for i in range(0, number):\n",
    "        nodes.append(pomegranate.State(word_distributions[i],\n",
    "            name=\"word\"+str(i+1)))\n",
    "\n",
    "    network = pomegranate.BayesianNetwork(\"Word Prediction\")\n",
    "    network.add_states(nodes)\n",
    "\n",
    "    # Add edges from current word to all later words\n",
    "    for i in range(0, number):\n",
    "        for j in range(i+1, number):\n",
    "            network.add_transition(nodes[i], nodes[j])\n",
    "\n",
    "    network.bake()\n",
    "\n",
    "    return network\n",
    "\n",
    "#\n",
    "# Make a prediction of the next word based on a string of words\n",
    "#\n",
    "def predict(network, number, s):\n",
    "    words = s.split(\" \")\n",
    "    num = number\n",
    "\n",
    "    # Start off with the desired number-1 of observations, but if we can't\n",
    "    # predict based on that, keep decreasing number until we can or until we\n",
    "    # find out we just can't predict.\n",
    "    while num > 1:\n",
    "        observations = {}\n",
    "\n",
    "        # Start at the last word and look backwards for our observations. But,\n",
    "        # to predict, we need the last node to be the next word, so don't\n",
    "        # observe all number of words in our network, just observe n-1 words.\n",
    "        for i, word in enumerate(words[-(num-1):]):\n",
    "            observations[\"word\"+str(i+1)] = word\n",
    "\n",
    "        # Update the beliefs in the network\n",
    "        try:\n",
    "            if debug:\n",
    "                print(\"Input string:\", s)\n",
    "                print(\"Observations:\", observations)\n",
    "\n",
    "            marginals = network.predict_proba(observations)\n",
    "\n",
    "            # Which node do we want to look at? If there are no previous words,\n",
    "            # look at the first one. If there was one, look at the second, etc.\n",
    "            # If there was n-1 or more, look at the last one.\n",
    "            prediction_node = min(len(words), num-1)\n",
    "\n",
    "            # Look at the node we care about to read off the new probability\n",
    "            # distribution\n",
    "            prediction = marginals[prediction_node].parameters[0]\n",
    "\n",
    "            # Sort the predictions in descending order\n",
    "            sorted_prediction = sorted(prediction.items(),\n",
    "                    key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "            return sorted_prediction\n",
    "\n",
    "        # If our observations \"aren't possible,\" i.e. they didn't occur in the\n",
    "        # training data then try again using one fewer word as an observation,\n",
    "        # but only if we haven't already used that number of words\n",
    "        except ZeroDivisionError:\n",
    "            if len(words) >= number-1:\n",
    "                num -= 1\n",
    "            else:\n",
    "                break\n",
    "        except ValueError:\n",
    "            if len(words) >= number-1:\n",
    "                num -= 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Couldn't make a prediction\n",
    "    return []\n",
    "\n",
    "#\n",
    "# Print the predictions\n",
    "#\n",
    "# Assuming input is sorted in descending order of most probable to least\n",
    "# probable\n",
    "#\n",
    "def printPrediction(predictions):\n",
    "    num = min(25, len(predictions))\n",
    "\n",
    "    if num == 0:\n",
    "        print(\"No prediction found\")\n",
    "    else:\n",
    "        # Print out the top ones that are greater than zero\n",
    "        for i in range(0, num):\n",
    "            if predictions[i][1] > 0:\n",
    "                print(\" \", predictions[i], sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the network from one of these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global debug\n",
    "debug = True\n",
    "files = [\n",
    "    'alices_adventures_in_wonderland.txt'\n",
    "]\n",
    "number = 3\n",
    "\n",
    "# Construct the network based on the input training files\n",
    "network = constructNetworkFromFiles(files[-1:], number)\n",
    "\n",
    "# Simplify calls below\n",
    "def p(s):\n",
    "    printPrediction(predict(network, number, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string: this is\n",
      "Observations: {'word2': 'is', 'word1': 'this'}\n",
      " ('May', 1.0)\n"
     ]
    }
   ],
   "source": [
    "p(\"this is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string: why did\n",
      "Observations: {'word2': 'did', 'word1': 'why'}\n",
      " ('they', 1.0)\n"
     ]
    }
   ],
   "source": [
    "p(\"why did\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string: they did not\n",
      "Observations: {'word2': 'not', 'word1': 'did'}\n",
      " ('like', 0.22222222222222196)\n",
      " ('quite', 0.07407407407407404)\n",
      " ('get', 0.07407407407407404)\n",
      " ('at', 0.07407407407407404)\n",
      " ('dare', 0.07407407407407404)\n",
      " ('venture', 0.07407407407407404)\n",
      " ('much', 0.03703703703703708)\n",
      " ('look', 0.03703703703703708)\n",
      " ('seem', 0.03703703703703708)\n",
      " ('appear', 0.03703703703703708)\n",
      " ('come', 0.03703703703703708)\n",
      " ('feel', 0.03703703703703708)\n",
      " ('notice', 0.03703703703703708)\n",
      " ('sneeze', 0.03703703703703708)\n",
      " ('answer', 0.03703703703703708)\n",
      " ('wish', 0.03703703703703708)\n",
      " ('see', 0.03703703703703708)\n"
     ]
    }
   ],
   "source": [
    "p(\"they did not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string: four score and\n",
      "Observations: {'word2': 'and', 'word1': 'score'}\n",
      "Input string: four score and\n",
      "Observations: {'word1': 'and'}\n",
      " ('the', 0.4845312646041715)\n",
      " ('she', 0.1728198897093195)\n",
      " ('then', 0.0732778764370508)\n",
      " ('was', 0.031965604262080775)\n",
      " ('a', 0.023927469856996182)\n",
      " ('Alice', 0.021030002804000518)\n",
      " ('as', 0.015795868772782582)\n",
      " ('began', 0.01570240209365373)\n",
      " ('said', 0.014580801944107002)\n",
      " ('looked', 0.011309468174595827)\n",
      " ('went', 0.010281334704178016)\n",
      " ('all', 0.007570801009440203)\n",
      " ('I', 0.005981867464249034)\n",
      " ('they', 0.005981867464249034)\n",
      " ('he', 0.005981867464249034)\n",
      " ('when', 0.005981867464249013)\n",
      " ('it', 0.004579867277315666)\n",
      " ('this', 0.004579867277315657)\n",
      " ('had', 0.004579867277315649)\n",
      " ('there', 0.0033648004486400767)\n",
      " ('that', 0.0033648004486400767)\n",
      " ('in', 0.0033648004486400767)\n",
      " ('very', 0.0028040003738667297)\n",
      " ('after', 0.002336666978222274)\n",
      " ('found', 0.002336666978222274)\n"
     ]
    }
   ],
   "source": [
    "p(\"four score and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string: with much\n",
      "Observations: {'word2': 'much', 'word1': 'with'}\n",
      "Input string: with much\n",
      "Observations: {'word1': 'much'}\n",
      " ('as', 0.1428571428571425)\n",
      " ('of', 0.1428571428571425)\n",
      " ('surprised', 0.06349206349206363)\n",
      " ('the', 0.06349206349206363)\n",
      " ('frightened', 0.06349206349206363)\n",
      " ('at', 0.06349206349206363)\n",
      " ('accustomed', 0.015873015873015876)\n",
      " ('overcome', 0.015873015873015876)\n",
      " ('out', 0.015873015873015876)\n",
      " ('to', 0.015873015873015876)\n",
      " ('pleased', 0.015873015873015876)\n",
      " ('already', 0.015873015873015876)\n",
      " ('so', 0.015873015873015876)\n",
      " ('confused', 0.015873015873015876)\n",
      " ('matter', 0.015873015873015876)\n",
      " ('larger', 0.015873015873015876)\n",
      " ('under', 0.015873015873015876)\n",
      " (\"right'\", 0.015873015873015876)\n",
      " ('evidence', 0.015873015873015876)\n",
      " ('pepper', 0.015873015873015876)\n",
      " ('care', 0.015873015873015876)\n",
      " ('farther', 0.015873015873015876)\n",
      " ('about', 0.015873015873015876)\n",
      " ('if', 0.015873015873015876)\n",
      " ('use', 0.015873015873015876)\n"
     ]
    }
   ],
   "source": [
    "p(\"with much\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string: and then\n",
      "Observations: {'word2': 'then', 'word1': 'and'}\n",
      " ('said', 0.07142857142857137)\n",
      " ('the', 0.07142857142857137)\n",
      " ('sat', 0.03571428571428575)\n",
      " (\"'we\", 0.03571428571428575)\n",
      " (\"I'll\", 0.03571428571428575)\n",
      " ('dipped', 0.03571428571428575)\n",
      " ('and', 0.03571428571428575)\n",
      " ('they', 0.03571428571428575)\n",
      " ('added', 0.03571428571428575)\n",
      " ('keep', 0.03571428571428575)\n",
      " ('another', 0.03571428571428575)\n",
      " ('she', 0.03571428571428575)\n",
      " ('turned', 0.03571428571428575)\n",
      " ('hurried', 0.03571428571428575)\n",
      " ('after', 0.03571428571428575)\n",
      " ('all', 0.03571428571428575)\n",
      " ('quietly', 0.03571428571428575)\n",
      " ('if', 0.03571428571428575)\n",
      " ('Alice', 0.03571428571428575)\n",
      " ('such', 0.03571428571428575)\n",
      " ('at', 0.03571428571428575)\n",
      " ('a', 0.03571428571428575)\n",
      " ('raised', 0.03571428571428575)\n",
      " ('treading', 0.03571428571428575)\n",
      " ('nodded', 0.03571428571428575)\n"
     ]
    }
   ],
   "source": [
    "p(\"and then\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
